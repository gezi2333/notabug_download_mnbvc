{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取仓库名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup  \n",
    "import re  \n",
    "import urllib.request, urllib.error  \n",
    "\n",
    "# 得到指定一个URL的网页内容\n",
    "def askURL(url):\n",
    "    head = {  \n",
    "        \"User-Agent\": \"Mozilla / 5.0(Windows NT 10.0; Win64; x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 80.0.3987.122  Safari / 537.36\"\n",
    "    }\n",
    "\n",
    "    request = urllib.request.Request(url, headers=head)\n",
    "    html = \"\"\n",
    "    try:\n",
    "        response = urllib.request.urlopen(request)\n",
    "        html = response.read().decode(\"utf-8\", errors='ignore')\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print(e.reason)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取第2页及之后仓库名\n",
    "baseurl = 'https://notabug.org/explore/repos?page=' # 截至2023年10月24日，notabug有897页仓库数据，在第2页及以后所有的url链接为此格式\n",
    "repo_names = []\n",
    "for i in range(2, 900):\n",
    "    url = baseurl + str(i) + '&q='\n",
    "    #print(url)\n",
    "    html = askURL(url)  # 保存获取到的网页源码\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # 使用 find_all 方法找到所有带有 class='name' 的 a 标签\n",
    "    links = soup.find_all('a', class_='name')\n",
    "    # 提取 href 属性值\n",
    "    href_values = [link.get('href') for link in links]\n",
    "    repo_names.extend(href_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取第1页仓库名\n",
    "url = 'https://notabug.org/explore/repos' # 第1页仓库url链接\n",
    "html = askURL(url)  \n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "links = soup.find_all('a', class_='name')\n",
    "href_values = [link.get('href') for link in links]\n",
    "repo_names.extend(href_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去重\n",
    "repo_names = list(set(repo_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"repo_names.txt\", \"w\") as file:\n",
    "    for item in repo_names:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取仓库对应下载链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件读取列表\n",
    "with open(\"repo_names.txt\", \"r\") as file:\n",
    "    repo_names = [line.strip() for line in file]\n",
    "base_url = 'https://notabug.org'\n",
    "download_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log = open(\"printlog.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, repo_name in enumerate(repo_names):\n",
    "    if i != 17228:\n",
    "        continue\n",
    "    url = base_url + repo_name\n",
    "    html = askURL(url) \n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    # 找到包含链接的div标签\n",
    "    dropdown_div = soup.find('div', class_='ui basic jump dropdown icon button')\n",
    "    \n",
    "    # 提取所有class为\"item\"的a标签\n",
    "    try:\n",
    "        items = dropdown_div.find_all('a', class_='item')\n",
    "        # 遍历每个a标签，打印链接和文本内容\n",
    "        for item in items:\n",
    "            link = item['href']\n",
    "            link = base_url + link\n",
    "            print(link, file=print_log)\n",
    "            download_url.append(link)\n",
    "    except AttributeError: # 仓库没有对应下载链接\n",
    "        print('FLAG', file=print_log)\n",
    "        print(repo_name, file=print_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'printlog.txt'\n",
    "with open(file_path, \"r\") as file:\n",
    "    download_url = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_urls = []\n",
    "for zip_url in download_url:\n",
    "    if len(zip_url.split(sep='/'))==7 and zip_url[-4:]=='.zip':\n",
    "        zip_urls.append(zip_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"zip_urls.txt\", \"w\") as file:\n",
    "    for item in zip_urls:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
